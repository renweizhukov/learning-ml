{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Training Custom Object Detector\n",
    "\n",
    "## 5.1 Two options for training\n",
    "\n",
    "(1) Use a pre-trained model and then use transfer learning to learn a new object.\n",
    "\n",
    "(2) Learn new objects from scratch.\n",
    "\n",
    "The benefit of transfer learning is that training can be much quicker, and the required data that you might need is much less. For this reason, we're going to be doing transfer learning here.\n",
    "\n",
    "## 5.2 TensorFlow pre-trained models\n",
    "\n",
    "### 5.2.1 [Configuring jobs documentation](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md)\n",
    "\n",
    "### 5.2.2 [Sample configurations](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)\n",
    "\n",
    "## 5.3 Train our object detector\n",
    "\n",
    "We select the mobilenet model because it is fast and we intend to do the real-time object detection.\n",
    "\n",
    "### 5.3.1 Download the configuration file and the checkpoint of mobilenet.\n",
    "\n",
    "(1) Configuration file\n",
    "\n",
    "```bash\n",
    "$ mkdir training\n",
    "$ cd training\n",
    "$ wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config\n",
    "```\n",
    "\n",
    "(2) Checkpoint\n",
    "\n",
    "```bash\n",
    "$ cd models/research/object_detection\n",
    "$ wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\n",
    "$ tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz\n",
    "```\n",
    "\n",
    "### 5.3.2 Modify the configuration file.\n",
    "\n",
    "(1) Search for all of the `PATH_TO_BE_CONFIGURED` points and change them.\n",
    "\n",
    "(2) Modify batch size.\n",
    "\n",
    "Currently, it is set to 24 in my configuration file. Other models may have different batch sizes. If you get a memory error, you can try to decrease the batch size to get the model to fit in your VRAM.\n",
    "\n",
    "(3) Change the checkpoint name/path, num_examples to 22 (for airplane) or 12 (for macaroni), and label_map_path: \"training/airplane-detection.pbtxt\" or \"training/macaroni-detection.pbtxt\".\n",
    "\n",
    "For airplane:\n",
    "\n",
    "```\n",
    "# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.\n",
    "# Users should configure the fine_tune_checkpoint field in the train config as\n",
    "# well as the label_map_path and input_path fields in the train_input_reader and\n",
    "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
    "# should be configured.\n",
    "\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 1\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      ssd_anchor_generator {\n",
    "        num_layers: 6\n",
    "        min_scale: 0.2\n",
    "        max_scale: 0.95\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.3333\n",
    "      }\n",
    "    }\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 300\n",
    "        width: 300\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      convolutional_box_predictor {\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.8\n",
    "        kernel_size: 1\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        conv_hyperparams {\n",
    "          activation: RELU_6,\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.00004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              stddev: 0.03\n",
    "              mean: 0.0\n",
    "            }\n",
    "          }\n",
    "          batch_norm {\n",
    "            train: true,\n",
    "            scale: true,\n",
    "            center: true,\n",
    "            decay: 0.9997,\n",
    "            epsilon: 0.001,\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: 'ssd_mobilenet_v1'\n",
    "      min_depth: 16\n",
    "      depth_multiplier: 1.0\n",
    "      conv_hyperparams {\n",
    "        activation: RELU_6,\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.00004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            stddev: 0.03\n",
    "            mean: 0.0\n",
    "          }\n",
    "        }\n",
    "        batch_norm {\n",
    "          train: true,\n",
    "          scale: true,\n",
    "          center: true,\n",
    "          decay: 0.9997,\n",
    "          epsilon: 0.001,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    loss {\n",
    "      classification_loss {\n",
    "        weighted_sigmoid {\n",
    "        }\n",
    "      }\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      hard_example_miner {\n",
    "        num_hard_examples: 3000\n",
    "        iou_threshold: 0.99\n",
    "        loss_type: CLASSIFICATION\n",
    "        max_negatives_per_positive: 3\n",
    "        min_negatives_per_image: 0\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_config: {\n",
    "  batch_size: 10\n",
    "  optimizer {\n",
    "    rms_prop_optimizer: {\n",
    "      learning_rate: {\n",
    "        exponential_decay_learning_rate {\n",
    "          initial_learning_rate: 0.004\n",
    "          decay_steps: 800720\n",
    "          decay_factor: 0.95\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "      decay: 0.9\n",
    "      epsilon: 1.0\n",
    "    }\n",
    "  }\n",
    "  fine_tune_checkpoint: \"ssd_mobilenet_v1_coco_2017_11_17/model.ckpt\"\n",
    "  from_detection_checkpoint: true\n",
    "  # Note: The below line limits the training process to 200K steps, which we\n",
    "  # empirically found to be sufficient enough to train the pets dataset. This\n",
    "  # effectively bypasses the learning rate schedule (the learning rate will\n",
    "  # never decay). Remove the below line to train indefinitely.\n",
    "  num_steps: 200000\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    ssd_random_crop {\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/airplane_train.record\"\n",
    "  }\n",
    "  label_map_path: \"training/airplane-detection.pbtxt\"\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 22\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  max_evals: 10\n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/airplane_test.record\"\n",
    "  }\n",
    "  label_map_path: \"training/airplane-detection.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "}\n",
    "```\n",
    "\n",
    "For macaroni:\n",
    "\n",
    "```\n",
    "# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.\n",
    "# Users should configure the fine_tune_checkpoint field in the train config as\n",
    "# well as the label_map_path and input_path fields in the train_input_reader and\n",
    "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
    "# should be configured.\n",
    "\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 1\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      ssd_anchor_generator {\n",
    "        num_layers: 6\n",
    "        min_scale: 0.2\n",
    "        max_scale: 0.95\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.3333\n",
    "      }\n",
    "    }\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 300\n",
    "        width: 300\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      convolutional_box_predictor {\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.8\n",
    "        kernel_size: 1\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        conv_hyperparams {\n",
    "          activation: RELU_6,\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.00004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              stddev: 0.03\n",
    "              mean: 0.0\n",
    "            }\n",
    "          }\n",
    "          batch_norm {\n",
    "            train: true,\n",
    "            scale: true,\n",
    "            center: true,\n",
    "            decay: 0.9997,\n",
    "            epsilon: 0.001,\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: 'ssd_mobilenet_v1'\n",
    "      min_depth: 16\n",
    "      depth_multiplier: 1.0\n",
    "      conv_hyperparams {\n",
    "        activation: RELU_6,\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.00004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            stddev: 0.03\n",
    "            mean: 0.0\n",
    "          }\n",
    "        }\n",
    "        batch_norm {\n",
    "          train: true,\n",
    "          scale: true,\n",
    "          center: true,\n",
    "          decay: 0.9997,\n",
    "          epsilon: 0.001,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    loss {\n",
    "      classification_loss {\n",
    "        weighted_sigmoid {\n",
    "        }\n",
    "      }\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      hard_example_miner {\n",
    "        num_hard_examples: 3000\n",
    "        iou_threshold: 0.99\n",
    "        loss_type: CLASSIFICATION\n",
    "        max_negatives_per_positive: 3\n",
    "        min_negatives_per_image: 0\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_config: {\n",
    "  batch_size: 10\n",
    "  optimizer {\n",
    "    rms_prop_optimizer: {\n",
    "      learning_rate: {\n",
    "        exponential_decay_learning_rate {\n",
    "          initial_learning_rate: 0.004\n",
    "          decay_steps: 800720\n",
    "          decay_factor: 0.95\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "      decay: 0.9\n",
    "      epsilon: 1.0\n",
    "    }\n",
    "  }\n",
    "  fine_tune_checkpoint: \"ssd_mobilenet_v1_coco_2017_11_17/model.ckpt\"\n",
    "  from_detection_checkpoint: true\n",
    "  # Note: The below line limits the training process to 200K steps, which we\n",
    "  # empirically found to be sufficient enough to train the pets dataset. This\n",
    "  # effectively bypasses the learning rate schedule (the learning rate will\n",
    "  # never decay). Remove the below line to train indefinitely.\n",
    "  num_steps: 200000\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    ssd_random_crop {\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/macaroni_train.record\"\n",
    "  }\n",
    "  label_map_path: \"training/macaroni-detection.pbtxt\"\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 12\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  max_evals: 10\n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"data/macaroni_test.record\"\n",
    "  }\n",
    "  label_map_path: \"training/macaroni-detection.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "}\n",
    "```\n",
    "\n",
    "### 5.3.3 Add `airplane-detection.pbtxt` and `macaroni-detection.pbtxt` in `training` dir.\n",
    "\n",
    "(1) `airplane-detection.pbtxt`:\n",
    "\n",
    "```\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'airplane'\n",
    "}\n",
    "```\n",
    "\n",
    "(2) `macaroni-detection.pbtxt`:\n",
    "\n",
    "```\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'macncheese'\n",
    "}\n",
    "```\n",
    "\n",
    "## 5.4 Train our model\n",
    "\n",
    "Note that \n",
    "\n",
    "* Before training our model, we need to copy `data` and `training` dirs to `models/research/object_detection`.\n",
    "* **The training script won't automatically stop and it has to be stopped by `Ctrl+C`.** When we see that the final loss is around 1 or at least smaller than 2 (usually more than 10000 steps), we should use `Ctrl+C` to manually stop the training. \n",
    "\n",
    "```bash\n",
    "# Add models/research and models/research/slim to the environment variable $PYTHONPATH\n",
    "$ cd models/research\n",
    "$ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "\n",
    "$ cd object_detection\n",
    "$ python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_airplane.config\n",
    "$ python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_macaroni.config \n",
    "```\n",
    "\n",
    "To view the training progress, use `tensorboard` and expand the `TotalLoss` diagram at http://127.0.0.1:6006.\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir='training'\n",
    "```\n",
    "\n",
    "## 5.5 Use PaperSpace to train our model\n",
    "\n",
    "### 5.5.1 Register a PaperSpace account\n",
    "\n",
    "https://www.paperspace.com/&R=1L988BL\n",
    "\n",
    "You will immediately receive $10 credit after your first login.\n",
    "\n",
    "### 5.5.2 Log into PaperSpace.\n",
    "\n",
    "### 5.5.3 Create a machine.\n",
    "\n",
    "(1) Select region as \"WEST COAST (CA1)\".\n",
    "\n",
    "(2) Select OS as the public template \"Ubuntu 16.04 ML-in-a-Box Desktop Edition (Beta)\".\n",
    "\n",
    "(3) Select machine as P4000 (hourly).\n",
    "\n",
    "(4) Disselect \"Auto Snapshot\".\n",
    "\n",
    "### 5.5.4 Launch the machine and set up the TensorFlow-GPU environment.\n",
    "\n",
    "(1) Install Cuda 9.0 and reboot.\n",
    "\n",
    "Cuda 8.0 is already installed on the machine but the latest version of TensorFlow-GPU requires Cuda 9.0. **Note that the Cuda version should be exactly 9.0 and Cuda 9.1 won't work with the latest version of TensorFlow-GPU (1.7.0).**\n",
    "\n",
    "* Remove the Cuda package which conflicts with Cuda 9.0.\n",
    "\n",
    "```bash\n",
    "$ sudo dpkg --purge cuda-repo-ubuntu1404\n",
    "```\n",
    "\n",
    "* Install Cuda 9.0\n",
    "\n",
    "https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=debnetwork\n",
    "\n",
    "```bash\n",
    "$ sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\n",
    "$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n",
    "$ sudo apt update\n",
    "$ sudo apt install cuda=9.0.176-1\n",
    "```\n",
    "\n",
    "* Check Cuda version.\n",
    "\n",
    "```bash\n",
    "$ nvcc --version\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2017 NVIDIA Corporation\n",
    "Built on Fri_Sep__1_21:08:03_CDT_2017\n",
    "Cuda compilation tools, release 9.0, V9.0.176\n",
    "```\n",
    "\n",
    "(2) Reboot and check GPU card info.\n",
    "\n",
    "```bash\n",
    "$ nvidia-smi\n",
    "Wed Apr  4 22:45:49 2018       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Quadro P4000        Off  | 00000000:00:05.0  On |                  N/A |\n",
    "| 46%   33C    P8     8W / 105W |    330MiB /  8119MiB |      4%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID   Type   Process name                             Usage      |\n",
    "|=============================================================================|\n",
    "|    0      2430      G   /usr/lib/xorg/Xorg                           152MiB |\n",
    "|    0      2748      G   /usr/bin/gnome-shell                         120MiB |\n",
    "|    0      3306      G   ...-token=D5964AC9D49D4EB49AE3F83AAB47DC22    44MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "(3) Install cuDNN for Cuda 9.0.\n",
    "\n",
    "https://developer.nvidia.com/rdp/cudnn-download\n",
    "http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-deb\n",
    "\n",
    "* Download the cuDNN deb files.\n",
    "\n",
    "** Note that to work with TensorFlow 1.5, we have to install cuDNN 7.0.** \n",
    "\n",
    "* Install the runtime library.\n",
    "\n",
    "```bash\n",
    "$ sudo dpkg -i libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb\n",
    "```\n",
    "\n",
    "* Install the developer library.\n",
    "\n",
    "```bash\n",
    "$ sudo dpkg -i libcudnn7-dev_7.0.5.15-1+cuda9.0_amd64.deb\n",
    "```\n",
    "\n",
    "* Install the code samples and the cuDNN Library User Guide.\n",
    "\n",
    "```bash\n",
    "$ sudo dpkg -i libcudnn7-doc_7.0.5.15-1+cuda9.0_amd64.deb\n",
    "```\n",
    "\n",
    "(4) Verify the cuDNN installation.\n",
    "\n",
    "* Copy the cuDNN sample to a writable path.\n",
    "\n",
    "```bash\n",
    "$ cp -r /usr/src/cudnn_samples_v7/ $HOME\n",
    "```\n",
    "\n",
    "* Go to the writable path.\n",
    "\n",
    "```bash\n",
    "$ cd $HOME/cudnn_samples_v7/mnistCUDNN\n",
    "```\n",
    "\n",
    "* Compile the mnistCUDNN sample.\n",
    "\n",
    "```bash\n",
    "$ make clean && make\n",
    "```\n",
    "\n",
    "* Run the mnistCUDNN sample.\n",
    "\n",
    "```bash\n",
    "$ ./mnistCUDNN\n",
    "```\n",
    "\n",
    "If cuDNN is properly installed and running on your Linux system, you will see a message similar to the following:\n",
    "\n",
    "```bash\n",
    "Test passed!\n",
    "```\n",
    "\n",
    "(5) Check the cuDNN version.\n",
    "\n",
    "```bash\n",
    "$ cat /usr/include/x86_64-linux-gnu/cudnn_v7.h | grep CUDNN_MAJOR -A 2\n",
    "#define CUDNN_MAJOR 7\n",
    "#define CUDNN_MINOR 1\n",
    "#define CUDNN_PATCHLEVEL 2\n",
    "--\n",
    "#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
    "\n",
    "#include \"driver_types.h\"\n",
    "```\n",
    "\n",
    "(6) Install TensorFlow-GPU.\n",
    "\n",
    "```bash\n",
    "$ pip install --upgrade pip\n",
    "\n",
    "# Need to upgrade the numpy version from 0xb to 0xc.\n",
    "$ pip install --upgrade numpy\n",
    "\n",
    "# The version of installed TensorFlow is 1.4 which is outdated.\n",
    "$ pip uninstall tensorflow\n",
    "\n",
    "# The latest version of TensorFlow is 1.7 but we are having an issue \n",
    "# of \"illegal instruction\" on Ubuntu 16.04LTS. For details, please see \n",
    "# https://github.com/tensorflow/tensorflow/issues/17411\n",
    "$ pip install tensorflow==1.5\n",
    "$ pip install tensorflow-gpu==1.5\n",
    "```\n",
    "\n",
    "(7) Verify the TensorFlow-GPU installation.\n",
    "\n",
    "```bash\n",
    "$ python\n",
    ">>> import tensorflow as tf\n",
    "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
    "  from ._conv import register_converters as _register_converters\n",
    ">>> print(tf.__version__)\n",
    "1.5.0\n",
    ">>> hello = tf.constant('Hello, TensorFlow!')\n",
    ">>> sess =tf.Session()\n",
    "2018-04-04 23:01:57.164099: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\n",
    "2018-04-04 23:01:57.251426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2018-04-04 23:01:57.251656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \n",
    "name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48\n",
    "pciBusID: 0000:00:05.0\n",
    "totalMemory: 7.93GiB freeMemory: 7.58GiB\n",
    "2018-04-04 23:01:57.251683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)\n",
    ">>> print(sess.run(hello))\n",
    "b'Hello, TensorFlow!'\n",
    "```\n",
    "\n",
    "### 5.5.5 Train our model\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/renweizhukov/learning-ml.git\n",
    "\n",
    "$ cd tensorflow/tensorflow-object-detection-api-tutorial/\n",
    "$ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "\n",
    "$ cd object_detection\n",
    "$ python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_airplane.config\n",
    "$ python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_macaroni.config \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
